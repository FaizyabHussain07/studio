# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file.
# This file is used to tell search engine crawlers which pages they can and cannot access.

User-agent: *
Allow: /
